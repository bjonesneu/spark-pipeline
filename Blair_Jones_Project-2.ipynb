{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 Notebook\n",
    "### Class: W205.5 - Data Engineering\n",
    "    Student: Blair Jones\n",
    "\n",
    "    Date: 23 Oct 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context\n",
    "The company has created a service that delivers assessments. Now lots of different customers (e.g., Pearson) want to publish their assessments on the service. This work is to prepare for data scientists who work for these customers to run queries on the data. \n",
    "\n",
    "## Objective / Problem Statement\n",
    "- Create a data pipeline to receive data from customers, transform it into a form suitable for usage by data scientists, and save it into storage that will be accessible to the data scientists.\n",
    "\n",
    "### Sample customer usage\n",
    "\n",
    "Typical business queries expected by our customers include:\n",
    "- Are a majority of students consistently achieving high scores on certain exams?  If so, it may be time to adjust the complexity level of the exam.\n",
    "- Which are the least popular exams?  Should they be redesigned, replaced or retired?\n",
    "- Which exam questions have the most wrong responses?  Should they be redesigned, replaced or retired?\n",
    "\n",
    "## Solution Approach\n",
    "- The architecture is intended to receive data from assessment companies, and transform and publish it in a way that the customer's data scientists can analyze the information.\n",
    "- A critical consideration is that our solution must protect each customer's data.  Each customer's data must be logically partitioned to prevent inadvertent access to the wrong information.\n",
    "- For now, we will consider that a user presenting credentials granting access to a specific customer's data will have unlimited access to that data.  Role-based access controls may be considered in the future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical component view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: gray'>\n",
    "    <img src='./images/pipeline-overall.svg' />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technology Choices (for in-scope components)\n",
    "\n",
    "#### Driving principles\n",
    "The solution requires low-latency, scalability and portability.  It must also be flexibility to adapt as new functional requirements are identified as customers add more datasets and require additional types of queries.\n",
    "\n",
    "\n",
    "#### Technologies\n",
    "- Data Sources (event logs)\n",
    "    - Customer API not in scope.  To be designed and implemented in another project.\n",
    "    \n",
    "    \n",
    "- Streaming Context\n",
    "    - Topic Queues: Kafka\n",
    "    - Data Transformation: Spark\n",
    "    \n",
    "    \n",
    "- Distributed Storage\n",
    "    - Files:  Hadoop (HDFS)\n",
    "    \n",
    "\n",
    "- Runtime Platform\n",
    "    - Containers: Docker, Docker-Compose.  All other technologies will be run inside Docker containers.  The use of Docker ensure portability to different hosting options.\n",
    "    - Hosting: Google Cloud Platform (GCP).  All components will be deployed to GCP for development and testing.  The choice of production environment can be made later.  \n",
    "\n",
    "\n",
    "- Query Tools\n",
    "    - Not in scope\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Design\n",
    "\n",
    "We will approach the design simultaneously from two directions:\n",
    "* From the customer data input side (event logs) to identify the input data that is currently available to us.\n",
    "* From the customer usage side (queries) to identify the way the output data will be used.  And to identify if any additional data should be requested or specified in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis of Incoming Data\n",
    "\n",
    "A sample dataset was provided to aid in the design and implementation of the solution.  We explored some preliminary questions to understand the current design.\n",
    "\n",
    "- How many assessments are in the dataset?\n",
    "- How many people took Learning Git?\n",
    "- What is the least common course taken? And the most common?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many assessments are in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3280\n"
     ]
    }
   ],
   "source": [
    "%cat assessment-attempts-20180128-121051-nested.json | jq '.[].exam_name' | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique exams are in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n"
     ]
    }
   ],
   "source": [
    "%cat assessment-attempts-20180128-121051-nested.json | jq '.[].exam_name' | sort | uniq | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note\n",
    "\n",
    "- While inspecting the list of exam names, we found an entry entitled \"Example Exam For Development and Testing oh yeahsdf\".  This looks like a special case not intended for production.  Through the use of additional queries we determined that there are a total of 5 entries with this name in the dataset, and they have a different data structure from the other entries.\n",
    "\n",
    "<span style='font-size: 1.2em; color: blue;'> Recommendation </span>\n",
    "- A policy should be developed for content publishers so that data quality is not degraded by inadvertent inclusion of non-production data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "%cat assessment-attempts-20180128-121051-nested.json | jq '.[].exam_name' | grep 'yeahsdf' | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many assessments were there for \"Learning Git\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394\n"
     ]
    }
   ],
   "source": [
    "%cat assessment-attempts-20180128-121051-nested.json | jq '.[] | select(.exam_name==\"Learning Git\") | .exam_name' | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the least common course taken? And the most common?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The least popular exam is  Learning to Visualize Data with D3.js\n",
      "The MOST popular exam is  Learning Git\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "cmd = \"cat assessment-attempts-20180128-121051-nested.json | jq '.[].exam_name' | sort | uniq -c | sort -n\"\n",
    "exam_count = subprocess.getoutput(cmd)\n",
    "exam_count = exam_count.split(\"\\n\")\n",
    "print('The least popular exam is ',exam_count[0].split('\"')[-2])\n",
    "print('The MOST popular exam is ',exam_count[-1].split('\"')[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Key findings from data exploration ***\n",
    "* The sample dataset may contain corrupt data.  On first inspection (conducted by loading the dataset into Pandas in a separate notebook) it seems that the detailed exam question responses do not tally with the given summary statistics for each test.\n",
    "* There are duplicate assessment ids in the sample file, however the detailed questions and responses differ.  This represents 170 entries out for the entire dataset of 3280 entries, over 38 assessment id's (keen id's\n",
    "* There sample dataset contains 5 entries for development/test data (description is \"Example Exam For Development and Testing oh yeahsdf\").  A policy is needed for whether or not these types of entries will be accepted in incoming data feeds and how they will be treated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics \n",
    "\n",
    "A Topic is a queue that, in its most simplistic form, takes in event data and makes that data available to various consumers.  The events stored can be of any level of granularity, held for any length of time, and consumed by anyone and by multiple parties.\n",
    "\n",
    "The selection of Topic(s) for this project is driven by business needs.  Specifically, we want to represent meaningful business entities that data scientists then analyze.\n",
    "\n",
    "This initial design provides a single Topic design for all customers.  In the future special topics may be designed for specific customers.\n",
    "\n",
    "**Input-side Perspective**\n",
    "\n",
    "> From the initial data exploration, some natural candidate entities are:\n",
    ">   * Exams - each Exam is represented with summary metadata and statistics.\n",
    ">   * Assessments - each individual Assessment across all Exams is represented with summary metadata and statistics.  The sample dataset is organized with 1 entry for each assessment completed.\n",
    ">   * Questions - each individual Question across all Assessments is represented, along with the detailed responses from exam-takers.  This entity contains some potentially interesting data related to timings and number of times a student selected an option, which enables analysis of user interactions with the assessment system.\n",
    "\n",
    "\n",
    "**Query-side Perspective**\n",
    "\n",
    "> Users are likely to need a view onto these entities:\n",
    ">    * Assessments - for purpose of analysing across exams, types of exams and assessments, within and across different families.  Users can summarize this entity to view Exams.\n",
    ">    * Questions - as described above.\n",
    ">    * Students - to test hypotheses related to student profiles, learning needs and assessment performance.  Non-identifiable data.\n",
    "\n",
    "These entities do cover the example customer queries described in the Problem Statement section of this document.\n",
    "\n",
    "\n",
    "<span style='font-size: 1.2em; color: blue;'> Recommendation </span>\n",
    "\n",
    "- Based on the format and contents of the sample file, we recommend moving forward with Assessments as the primary Input Topic.  This will be implemented in Kafka.\n",
    "\n",
    "- For the data that will be used to support queries, we will use Assessments and Questions as the primary Entities.  These will be stored in Hadoop.\n",
    "\n",
    "- We also recommend including *non-identifiable* Student Profile data in the future specification of minimum incoming data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation\n",
    "\n",
    "The Exams / Assessments data is extracted easily from the incoming data stream.\n",
    "\n",
    "Questions are more challenging.  Preliminary data exploration indicates that the data is deeply nested within each assessment, and can contain a variable number of questions.\n",
    "\n",
    "The objective is to extract each question as a single row, but maintain a link back to the original exam from which it came.  Other metadata could be extracted as well.  But once the basic task of extracting questions and their link to the exam is accomplished, extracting other fields becomes trivial.\n",
    "\n",
    "For the purpose of this project, only the question and the link to the main exam are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Storage\n",
    "\n",
    "Storage in Hadoop is simplified after the transformations achieved with Spark.  We simply save the dataframe generated through the transformations into an appropriately named Hadoop fileset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Kafka Topic\n",
    "\n",
    "**Topic Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created topic assessments.\n"
     ]
    }
   ],
   "source": [
    "!docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: assessments\tPartitionCount: 1\tReplicationFactor: 1\tConfigs: \n",
      "\tTopic: assessments\tPartition: 0\tLeader: 1\tReplicas: 1\tIsr: 1\n"
     ]
    }
   ],
   "source": [
    "!docker-compose exec kafka kafka-topics --describe --topic assessments --zookeeper zookeeper:32181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample dataset load into Kafka Topic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produced messages.\n"
     ]
    }
   ],
   "source": [
    "!docker-compose exec mids bash -c \"cat /w205/project-2-bjonesneu/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessments && echo 'Produced messages.'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3281\n"
     ]
    }
   ],
   "source": [
    "!docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t assessments -o beginning -e\" | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Hadoop Storage\n",
    "\n",
    "**Verify Status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxrwxrwt   - mapred mapred              0 2018-02-06 18:27 /tmp/hadoop-yarn\n",
      "drwx-wx-wx   - root   supergroup          0 2020-10-19 15:39 /tmp/hive\n"
     ]
    }
   ],
   "source": [
    "!docker-compose exec cloudera hadoop fs -ls /tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Spark Stream\n",
    "\n",
    "This section is executed using the CLI.  The code used is copied here for reference.\n",
    "\n",
    "First, we launch the pyspark interface on the Spark container.\n",
    "\n",
    "```code\n",
    "CLI:   docker-compose exec spark pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we run these commands inside pyspark to read data from the kafka topic, cast it from binary to string, and convert from string to json and from there to a dataframe.\n",
    "\n",
    "```\n",
    "PYSPARK:  \n",
    "    raw_assessments = spark \\\n",
    "      .read \\\n",
    "      .format(\"kafka\") \\\n",
    "      .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "      .option(\"subscribe\",\"assessments\") \\\n",
    "      .option(\"startingOffsets\", \"earliest\") \\\n",
    "      .option(\"endingOffsets\", \"latest\") \\\n",
    "      .load() \n",
    "\n",
    "    raw_assessments.cache()\n",
    "\n",
    "    raw_assessments.printSchema()\n",
    "\n",
    "    assessments = raw_assessments.select(raw_assessments.value.cast('string'))\n",
    "\n",
    "    from pyspark.sql import Row\n",
    "    import json\n",
    "    extracted_assessments = assessments.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()\n",
    "    extracted_assessments.printSchema()\n",
    "```\n",
    "\n",
    "Output from extracted_assessments.printSchema()\n",
    "```\n",
    "        root\n",
    "         |-- base_exam_id: string (nullable = true)\n",
    "         |-- certification: string (nullable = true)\n",
    "         |-- exam_name: string (nullable = true)\n",
    "         |-- keen_created_at: string (nullable = true)\n",
    "         |-- keen_id: string (nullable = true)\n",
    "         |-- keen_timestamp: string (nullable = true)\n",
    "         |-- max_attempts: string (nullable = true)\n",
    "         |-- sequences: map (nullable = true)\n",
    "         |    |-- key: string\n",
    "         |    |-- value: array (valueContainsNull = true)\n",
    "         |    |    |-- element: map (containsNull = true)\n",
    "         |    |    |    |-- key: string\n",
    "         |    |    |    |-- value: boolean (valueContainsNull = true)\n",
    "         |-- started_at: string (nullable = true)\n",
    "         |-- user_exam_id: string (nullable = true)\n",
    "```\n",
    "\n",
    "As observed with the *printSchema* method and confirmed from initial exploration on dataset, the data contains a nested structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessments Entity\n",
    "\n",
    "We next use SparkSQL to interact with the data in a nicer way.\n",
    "\n",
    "```\n",
    "PYSPARK:  extracted_assessments.registerTempTable('assessments')\n",
    "```\n",
    "\n",
    "We select key fields and exclude the test data identified in the sample dataset.\n",
    "\n",
    "```code\n",
    "PYSPARK:  \n",
    "    assessment_info = spark.sql(\"select base_exam_id, exam_name, certification, started_at from assessments WHERE NOT (exam_name LIKE '%yeahsdf%')\")\n",
    "\n",
    "    assessment_info.select('base_exam_id').distinct().count()\n",
    "```\n",
    "\n",
    "There are 106 unique exams within the sample dataset.  We could save just the data for these exams out to Hadoop along with the last time the test was taken, but it would remove the information related to how often the exams are used, which can be useful.  So we will save the full set of assessment data related to exams to Hadoop.\n",
    "\n",
    "At this point it would be useful to extract \"counts\" metadata to store at the overall assessment level.  \"Counts\" is a field nested within the \"sequences\" field at the root level.  It contains the summary values of assessment results for a single assessment (for example:  # of correct/incorrect/incomplete answers).  Using spark.sql to select sequences.counts yields null values because the mapping function above finds inconsistencies in counts across the entire dataset and ignores the field.  The technique to extract this type of information is implemented in the next section but is not used in this section for brevity.\n",
    "\n",
    "We next save the data to Hadoop.\n",
    "```code\n",
    "PYSPARK:  assessment_info.write.parquet(\"/tmp/assessment_info\")\n",
    "```\n",
    "\n",
    "And then verify the operation completed successfully.\n",
    "\n",
    "```code\n",
    "CLI:  docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "CLI:  docker-compose exec cloudera hadoop fs -ls /tmp/assessment_info\n",
    "```\n",
    "\n",
    "This shows that a file for **assessment_info** was created and that the internal partition contains data.  We will demonstrate use of the data in Hadoop in another section of this notebook.\n",
    "\n",
    "Please note that, per recommendation, in the future for production usage a unique client identifier should be added to the dataset to logically partition the data for access controls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions Entity\n",
    "\n",
    "In this section we extract all questions from each assessment, and store each as an individual row with a reference to the base_exam_id.  This allows data scientists to run analysis, such as determining level of difficulty, time spent per question, frequency of changing answers, etc.\n",
    "\n",
    "Please note that for this section we use the entire sample dataset, including the 5 \"development\" assessments.\n",
    "\n",
    "```code\n",
    "PYSPARK:  \n",
    "    from pyspark.sql.functions import udf\n",
    "    from pyspark.sql.types import *\n",
    "    from pyspark.sql.functions import explode\n",
    "\n",
    "    @udf(ArrayType(StringType()))\n",
    "    def flatten_questions(assessment):\n",
    "        assessment_json = json.loads(assessment)\n",
    "        questions_flattened = []\n",
    "        for question in assessment_json['sequences']['questions']:\n",
    "            questions_flattened.append(question)\n",
    "        return questions_flattened\n",
    "\n",
    "    # This line was used to test the \"python\" version (without @udf) of the flatten function\n",
    "    # test = flatten_questions('{\"base_exam_id\":999, \"sequences\": {\"questions\": [\"a\", \"b\", \"c\"], \"counts\": 123} }')\n",
    "\n",
    "    @udf('string')\n",
    "    def flatten_ids(assessment):\n",
    "        assessment_json = json.loads(assessment)\n",
    "        return assessment_json['base_exam_id']\n",
    "\n",
    "    flattened_questions = raw_assessments \\\n",
    "        .select(raw_assessments.value.cast('string').alias('raw')) \\\n",
    "        .withColumn('q_array', flatten_questions('raw')) \\\n",
    "        .withColumn('base_exam_id', flatten_ids('raw')) \\\n",
    "        .withColumn(\"question\", explode('q_array')) \\\n",
    "        .drop('q_array')\n",
    "    flattened_questions.show(2)\n",
    "    flattened_questions.count()\n",
    "```\n",
    "\n",
    "The 3280 assessments in the sample dataset were converted into 14,717 questions.\n",
    "\n",
    "The output was then written to Hadoop and verified.\n",
    "\n",
    "```code\n",
    "PYSPARK:  flattened_questions.write.parquet(\"/tmp/assessment_questions\")\n",
    "CLI:      docker-compose exec cloudera hadoop fs -ls /tmp/assessment_questions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='font-size: 1.5em; color:blue'><b>Special Note</b></span>\n",
    "\n",
    "The solution provided for the \"Questions\" entity works and addresses the problem of storing the list of questions and their relationship to the exams from which they came.\n",
    "    \n",
    "The nested structure of \"questions\" within the assessments presented special challenges for breaking out those questions individually in a usable form.  The solution addresses several issues:\n",
    "    \n",
    "- Flattening the \"deeply\" nested question structure\n",
    "- Retaining top-level key data (base_exam_id) to retain link between individual questions and base exam\n",
    "- Data type conversions to make use of Spark native features\n",
    "    \n",
    "My solution was inspired by stackoverflow research, a very helpful conversation with Mark and Taylor in office hours, and a code snippet from week 11 sync-slides.md they pointed out.\n",
    "    \n",
    "In office hours we discussed some alternatives, including using jq or Pandas to achieve the flattening.  I chose to stay with Spark for learning purposes and to mimic the need to implement this as a single, simplified pipeline.\n",
    "    \n",
    "However, I did not examine the performance characteristics of this solution.  Given the small size of the dataset used, I would test this for scalability before using it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Query\n",
    "\n",
    "Here is a sample query that may be helpful for future data science team users of this platform.\n",
    "\n",
    "### Query - What is the last date each exam was taken?\n",
    "\n",
    "```spark\n",
    "last_used_exams = spark.sql(\"SELECT a.base_exam_id, a.exam_name, a.started_at FROM assessments a INNER JOIN (SELECT base_exam_id, max(started_at) AS maxdate FROM assessments GROUP BY base_exam_id) am ON a.base_exam_id = am.base_exam_id AND a.started_at = am.maxdate\")\n",
    "```\n",
    "\n",
    "```sql\n",
    "SELECT a.base_exam_id, a.exam_name, a.started_at\n",
    "FROM assessments a \n",
    "INNER JOIN (\n",
    "    SELECT base_exam_id, max(started_at) AS maxdate \n",
    "    FROM assessments \n",
    "    GROUP BY base_exam_id\n",
    ") am ON a.base_exam_id = am.base_exam_id AND a.started_at = am.maxdate\n",
    "```\n",
    "\n",
    "Output\n",
    "\n",
    "```\n",
    "+--------------------+--------------------+--------------------+                \n",
    "|        base_exam_id|           exam_name|          started_at|\n",
    "+--------------------+--------------------+--------------------+\n",
    "|76a682de-6f0c-11e...|Learning iPython ...|2018-01-23T21:46:...|\n",
    "|a8dedd1d-0f67-4f4...|Learning C# Desig...|2018-01-27T04:01:...|\n",
    "|0fed9e6e-6438-464...|          Great Bash|2018-01-26T18:37:...|\n",
    "|b2264d14-7699-11e...|        Learning DNS|2018-01-27T18:06:...|\n",
    "|479f39cc-70a9-11e...|Learning Data Mod...|2018-01-16T10:15:...|\n",
    "+--------------------+--------------------+--------------------+\n",
    "only showing top 5 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Thoughts\n",
    "\n",
    "I struggled quite a bit with the way that Spark reads data and converts data types, sometimes converting in a way that I was unable to override.  Ultimately this made it extremely difficult to manipulate the data as compared to other programming languages such as Java or Python.  While Spark brings the possibility of massive scalability, I find the challenge of working around the type-casting approach to be extremely time consuming.  There were many many examples of solutions for a myriad of casting issues on stackoverflow, however they seemed extremely specialized and seemed more like \"tricks\".  I'm unsure that solutions like that should be used in production systems just from the perspective of maintainability.\n",
    "\n",
    "I wanted to store some fields in Hadoop as Maps for the later purpose of performing Reduce functions for simple calculations, such as sums, counts, etc.  At the end I was unable to achieve this to the degree that I originally planned.\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
